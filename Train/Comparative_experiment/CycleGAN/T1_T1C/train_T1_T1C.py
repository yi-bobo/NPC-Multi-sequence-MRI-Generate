import sys
import os
import torch
import random
import logging
import warnings
import numpy as np
from collections import defaultdict
from torch.utils.data import DataLoader

# è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥
warnings.filterwarnings('ignore')    # å¿½ç•¥è­¦å‘Š
sys.path.append('/data1/weiyibo/NPC-MRI/Code/NPC-Multi-sequence-MRI-Generate/')
from Model.CycleGAN_model import CycleGANModel
from Dataset.patch_dataset import npy_3D_dataset
from Utils.path_util import create_directories
from Utils.config_util import load_yaml_config, log_namespace
from Utils.logging_util import setup_logging, save_loss_csv
from Utils.save_load_model_util import save_model, load_model


def set_random_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)

def initialize_dataloader(opt):
    """åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨"""
    logging.info("========== â³åŠ è½½æ•°æ®é›†â³ ==========")
    train_dataset = npy_3D_dataset(opt.data, mode='train')
    train_loader = DataLoader(
        train_dataset, 
        batch_size=opt.train.batch_size, 
        shuffle=True, 
        num_workers=opt.train.num_workers, 
        drop_last=True
    )
    logging.info(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, æ‰¹æ¬¡æ•°: {len(train_loader)}")

    val_dataset = npy_3D_dataset(opt.data, mode='val')
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=1)
    logging.info(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}, æ‰¹æ¬¡æ•°: {len(val_loader)}")

    test_dataset = npy_3D_dataset(opt.data, mode='test')
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1)
    logging.info(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}, æ‰¹æ¬¡æ•°: {len(test_loader)}")
    logging.info("========== âŒ›æ•°æ®é›†åŠ è½½å®Œæ¯•âŒ› ==========")

    return train_loader, val_loader, test_loader


def validate_model(model, val_loader, epoch, opt, best_metrics, model_components):
    """éªŒè¯æ¨¡å‹"""
    logging.info(f"ğŸ˜ƒå¼€å§‹éªŒè¯epoch{epoch}æ¨¡å‹")
    mae_targ_list, ssim_targ_list, psnr_targ_list = [], [], []
    mae_input_list, ssim_input_list, psnr_input_list = [], [], []

    with torch.no_grad():
        for i, data in enumerate(val_loader):
        # for i, data in zip(range(2), val_loader):

            input, target, patient_id, data_mapping = model.set_input(data)
            input, target, pred_target, pred_input, mae_targ, SSIM_targ, PSNR_targ, mae_input, SSIM_input, PSNR_input = model.val(input, target)
            
            mae_targ_list.append(mae_targ)
            ssim_targ_list.append(SSIM_targ)
            psnr_targ_list.append(PSNR_targ)
            mae_input_list.append(mae_input)
            ssim_input_list.append(SSIM_input)
            psnr_input_list.append(PSNR_input)

            # ä¿å­˜å•ä¸ªæ ·æœ¬éªŒè¯ç»“æœ
            val_cal_dict = {'patient_id': patient_id, 'MAE_targ': mae_targ, 'SSIM_targ': SSIM_targ, 'PSNR_targ': PSNR_targ, 
                            'MAE_input': mae_input, 'SSIM_input': SSIM_input, 'PSNR_input': PSNR_input}
            logging.info(f"éªŒè¯é›†epoch_{epoch}æ¨¡å‹çš„patient_id:{patient_id}çš„éªŒè¯ç»“æœ:"
                         f"MAE_targ:{mae_targ:.4f}, SSIM_targ:{SSIM_targ:.4f}, PSNR_targ:{PSNR_targ:.4f}, "
                         f"MAE_input:{mae_input:.4f}, SSIM_input:{SSIM_input:.4f}, PSNR_input:{PSNR_input:.4f}")
            save_loss_csv(
                file_path=opt.path.val_metric_csv_path,
                epoch=epoch,
                header=['patient_id', 'MAE_targ', 'SSIM_targ', 'PSNR_targ', 'MAE_input', 'SSIM_input', 'PSNR_input'],
                loss_dict=val_cal_dict
            )

            # save_path_dir = os.path.join(opt.path.img_path_dir, 'val', f"epoch_{epoch}", patient_id)
            # os.makedirs(save_path_dir, exist_ok=True)
            # model.plot(input, target, pred_target, data_mapping, save_path_dir)

    # è®¡ç®—éªŒè¯é›†å¹³å‡å€¼ä¸æ ‡å‡†å·®
    metrics_avg = {
        'MAE_targ': (np.mean(mae_targ_list), np.std(mae_targ_list)),
        'SSIM_targ': (np.mean(ssim_targ_list), np.std(ssim_targ_list)),
        'PSNR_targ': (np.mean(psnr_targ_list), np.std(psnr_targ_list)),
        'MAE_input': (np.mean(mae_input_list), np.std(mae_input_list)),
        'SSIM_input': (np.mean(ssim_input_list), np.std(ssim_input_list)),
        'PSNR_input': (np.mean(psnr_input_list), np.std(psnr_input_list)),
        }
    # æ ¼å¼åŒ–è¾“å‡ºç»“æœ
    metrics_str = (
        f"MAE_targ: {metrics_avg['MAE_targ'][0]:.6f} Â± {metrics_avg['MAE_targ'][1]:.6f}, "
        f"SSIM_targ: {metrics_avg['SSIM_targ'][0]:.6f} Â± {metrics_avg['SSIM_targ'][1]:.6f}, "
        f"PSNR_targ: {metrics_avg['PSNR_targ'][0]:.6f} Â± {metrics_avg['PSNR_targ'][1]:.6f}, "
        f"MAE_input: {metrics_avg['MAE_input'][0]:.6f} Â± {metrics_avg['MAE_input'][1]:.6f}, "
        f"SSIM_input: {metrics_avg['SSIM_input'][0]:.6f} Â± {metrics_avg['SSIM_input'][1]:.6f}, "
        f"PSNR_input: {metrics_avg['PSNR_input'][0]:.6f} Â± {metrics_avg['PSNR_input'][1]:.6f}"
    )
    logging.info(f"éªŒè¯é›†epoch_{epoch}æ¨¡å‹çš„éªŒè¯ç»“æœ: {metrics_str}")
    
    # ä¿å­˜éªŒè¯é›†ç»“æœ
    save_loss_csv(
        file_path=opt.path.val_avg_metric_csv_path,
        epoch=epoch,
        header=['epoch', 'MAE_targ', 'SSIM_targ', 'PSNR_targ', 'MAE_input', 'SSIM_input', 'PSNR_input'],
        loss_dict={key: f"{val[0]:.4f} Â± {val[1]:.4f}" for key, val in metrics_avg.items()}
    )

    # æ›´æ–°æœ€ä½³æŒ‡æ ‡å¹¶ä¿å­˜æ¨¡å‹
    for metric, (avg, _) in metrics_avg.items():
        if avg > best_metrics[metric]:
            best_metrics[metric] = avg
            save_model(
                model_components=model_components,
                epoch=epoch,
                save_dir=opt.path.checkpoint_path_dir,
                file_name=f"best_{metric.lower()}.pth"
            )
            logging.info(f"ğŸ‰ä¿å­˜epoch{epoch}æ¨¡å‹ï¼Œ{metric}æœ€ä½³æ¨¡å‹")

    return best_metrics

def random_sliding_window_image(model, data, patch_size, overlap):
    """
    å¯¹ 3D å›¾åƒè¿›è¡Œéšæœºè£å‰ªï¼Œå¹¶è®¡ç®—æ¯ä¸ªå›¾åƒæ‰€æœ‰éšæœºå—çš„æŸå¤±å‡å€¼ã€‚

    Args:
        model: æ¨¡å‹å¯¹è±¡ï¼ŒåŒ…å« `set_input` å’Œå‰å‘ä¼ æ’­é€»è¾‘ã€‚
        data: è¾“å…¥æ•°æ®ï¼ŒåŒ…å« x_0, x_T, txt_con, img_con ç­‰ã€‚
        patch_size (tuple): æ¯ä¸ªå—çš„å¤§å° (patch_d, patch_h, patch_w)ã€‚
        overlap (float): ç”¨äºè®¡ç®—éšæœºè£å‰ªå—çš„æ•°é‡ (0 <= overlap < 1)ã€‚
        num_random_crops (int): æ¯å¼ å›¾åƒè¦éšæœºè£å‰ªçš„å—æ•°é‡ã€‚

    Returns:
        avg_loss (float): æ‰€æœ‰éšæœºå—çš„æŸå¤±å‡å€¼ã€‚
        avg_loss_dict (dict): æ‰€æœ‰éšæœºå—çš„æŸå¤±å­—å…¸å‡å€¼ã€‚
    """
    # æå–æ¨¡å‹è¾“å…¥
    input, target, _, _ = model.set_input(data)

    b, c, d, h, w = input.shape
    patch_d, patch_h, patch_w = patch_size

    # ç¡®ä¿å›¾åƒå°ºå¯¸è¶³å¤Ÿè£å‰ª
    assert d >= patch_d and h >= patch_h and w >= patch_w, "å›¾åƒå°ºå¯¸å¿…é¡»å¤§äºå—å¤§å°"
    num_random_crops = 8   # éšæœºè£å‰ªå—çš„æ•°é‡ï¼Œè¿™é‡Œè®¾ç½®ä¸º 4ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´

    # åˆå§‹åŒ–æŸå¤±ç´¯åŠ 
    total_loss_dict = defaultdict(lambda:0.0)

    for _ in range(num_random_crops):
        # éšæœºè£å‰ªèµ·å§‹ä½ç½®ï¼Œç¡®ä¿ä¸ä¼šè¶Šç•Œ
        start_d = random.randint(0, d - patch_d)
        start_h = random.randint(0, h - patch_h)
        start_w = random.randint(0, w - patch_w)

        # è£å‰ªå—
        input_patch = input[:, :, start_d:start_d + patch_d, start_h:start_h + patch_h, start_w:start_w + patch_w]
        target_patch = target[:, :, start_d:start_d + patch_d, start_h:start_h + patch_h, start_w:start_w + patch_w]
        
        # ç¡®ä¿å—åœ¨è®¾å¤‡ä¸Š
        input_patch = input_patch.to(input.device)
        target_patch = target_patch.to(input.device)

        # è®¡ç®—å•ä¸ªå—çš„æŸå¤±
        loss_dict = model.optimize_parameters(input_patch, target_patch)

        # ç´¯åŠ æŸå¤±å­—å…¸ä¸­çš„æ¯é¡¹
        for key in loss_dict:
            total_loss_dict[key] += loss_dict[key]

    # è®¡ç®—æ‰€æœ‰éšæœºå—çš„å¹³å‡æŸå¤±
    avg_loss_dict = {key: value / num_random_crops for key, value in total_loss_dict.items()}

    return avg_loss_dict

def main():
    # 1ï¸âƒ£ åŠ è½½é…ç½®
    config_path = "./Config/Comparative_experiment/CycleGAN/T1_T1C.yaml"
    opt = load_yaml_config(config_path)

    # 1.1 è®¾ç½®éšæœºç§å­ä¸ä¿å­˜è·¯å¾„
    set_random_seed(opt.train.seed)
    create_directories(opt)

    # 2ï¸âƒ£ é…ç½®æ—¥å¿—
    setup_logging(opt)
    logging.info("========== â³è®­ç»ƒé…ç½®å‚æ•°â³ ==========")
    log_namespace(opt)

    # 3ï¸âƒ£ é€‰æ‹©è®¾å¤‡
    if opt.train.multi_gpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = f'{opt.train.gpu}'
        opt.train.device = torch.device(f"cuda:0" if torch.cuda.is_available() else "cpu")
        available_gpus = torch.cuda.device_count()
        opt.train.batch_size = int(opt.train.batch_size * available_gpus)  # åŠ¨æ€è°ƒæ•´ batch_size
        logging.info(f"ä½¿ç”¨ {available_gpus} ä¸ª GPU, æ›´æ–° batch_size ä¸º {opt.train.batch_size}")
    else:
        opt.train.device = torch.device(f"cuda:{opt.train.gpu}" if torch.cuda.is_available() else "cpu")
        logging.info(f"ä½¿ç”¨è®¾å¤‡: {opt.train.device}")

    # 4ï¸âƒ£ åŠ è½½æ•°æ®é›†
    train_loader, val_loader, test_loader = initialize_dataloader(opt)

    # 5ï¸âƒ£ åˆ›å»ºæ¨¡å‹
    model = CycleGANModel(opt).to(opt.train.device)
    model_components = {'netG_A2B': model.netG_A2B.state_dict(), 'netG_B2A': model.netG_B2A.state_dict(),
                         'netD_A2B': model.netD_A2B.state_dict(), 'netD_B2A': model.netD_B2A.state_dict(),
                         'optimizer_G': model.optimizer_G.state_dict(), 'optimizer_D': model.optimizer_D.state_dict()}
    if opt.train.continue_train:
        logging.info("åŠ è½½å·²ä¿å­˜æ¨¡å‹å‚æ•°")
        val_model = {'netG_A2B': model.netG_A2B, 'netG_B2A': model.netG_B2A, 'netD_A2B': model.netD_A2B, 'netD_B2A': model.netD_B2A,
                     'optimizer_G': model.optimizer_G, 'optimizer_D': model.optimizer_D}
        opt.train.epoch_start = load_model(val_model, checkpoint_path=opt.path.checkpoint_path, device=opt.train.device)
    best_metrics = {metric: 0 for metric in ['MAE_targ', 'SSIM_targ', 'PSNR_targ', 'MAE_input', 'SSIM_input', 'PSNR_input']}

    # 6ï¸âƒ£ å¼€å§‹è®­ç»ƒ
    logging.info("========== â³å¼€å§‹è®­ç»ƒâ³ ==========")
    for epoch in range(opt.train.epoch_start + 1, opt.train.max_epochs + 1):
        loss_dict = defaultdict(lambda:0.0) # åˆå§‹åŒ–æ¯ä¸ªæŸå¤±çš„ç´¯åŠ å€¼ä¸º0
        loss_sums = defaultdict(lambda:0.0) # åˆå§‹åŒ–æ¯ä¸ªæŸå¤±çš„ç´¯åŠ å€¼ä¸º0

        for i, data in enumerate(train_loader):
        # for i, data in zip(range(2), train_loader):
            loss_dict = random_sliding_window_image(model, data, opt.data.patch_image_shape, opt.data.overlap)
            loss_sums = {key: loss_sums[key] + loss_dict[key] for key in loss_dict.keys()}
            # ç´¯åŠ æ¯ä¸ªæŸå¤±å‡½æ•°çš„å€¼
            if i % opt.train.log_freq == 0:
                lr_G = model.optimizer_G.param_groups[0]['lr']
                lr_D = model.optimizer_D.param_groups[0]['lr']
                train_str = f"epoch:{epoch}|{opt.train.max_epochs}; batch:{i+1}/{len(train_loader)}; Lr_G:{lr_G:.7f}; Lr_D:{lr_D:.7f}; " + ", ".join([f"{key}:{value:.6f}" for key, value in loss_dict.items()])
                logging.info(train_str)

        # è®¡ç®—å¹³å‡æŸå¤±å¹¶ä¿å­˜
        avg_loss_dict = {key: loss_sums[key] / len(train_loader) for key in loss_sums.keys()}
        avg_loss_str = f"epoch:{epoch}|{opt.train.max_epochs};" + ", ".join([f"{key}:{value:.6f}" for key, value in avg_loss_dict.items()])
        logging.info(avg_loss_str)
        save_loss_csv(opt.path.train_avg_loss_csv_path, epoch,  ['epoch'] + list(avg_loss_dict.keys()), {'epoch': epoch, **avg_loss_dict})

        # å­¦ä¹ ç‡è°ƒåº¦ä¸æ¨¡å‹ä¿å­˜
        model.scheduler_G.step()
        model.scheduler_D.step()
        save_model(model_components, epoch, opt.path.checkpoint_path_dir, file_name=f"latest.pth")

        # éªŒè¯æ¨¡å‹
        if epoch % opt.train.val_freq == 0:
            torch.cuda.empty_cache()
            model.to(opt.train.device)
            best_metrics = validate_model(model, val_loader, epoch, opt, best_metrics, model_components)


if __name__ == '__main__':
    main()